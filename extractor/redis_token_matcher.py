#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Redis Token åŒ¹é…å™¨
åœ¨æ¶ˆæ¯ä¸­å¿«é€ŸåŒ¹é… Redis ä¸­å·²ä¿å­˜çš„ token_symbol
"""

import re
import redis
from typing import List, Dict, Set, Tuple
from datetime import datetime


class RedisTokenMatcher:
    """Redis Token åŒ¹é…å™¨ - ç”¨æ­£åˆ™å¿«é€ŸåŒ¹é…æ¶ˆæ¯ä¸­çš„å·²çŸ¥ token"""
    
    def __init__(self, redis_host='127.0.0.1', redis_port=6379, redis_db=0,
                 redis_set_key='nlpmeme:tokens:all', redis_key_prefix='nlpmeme:token:'):
        """
        åˆå§‹åŒ–åŒ¹é…å™¨
        
        Args:
            redis_host: Redis ä¸»æœº
            redis_port: Redis ç«¯å£
            redis_db: Redis æ•°æ®åº“ç¼–å·
            redis_set_key: å­˜å‚¨æ‰€æœ‰ token çš„é›†åˆ key
            redis_key_prefix: Token è¯¦æƒ…çš„ key å‰ç¼€
        """
        self.redis_host = redis_host
        self.redis_port = redis_port
        self.redis_db = redis_db
        self.redis_set_key = redis_set_key
        self.redis_key_prefix = redis_key_prefix
        
        # è¿æ¥ Redis
        try:
            self.redis_client = redis.Redis(
                host=redis_host,
                port=redis_port,
                db=redis_db,
                decode_responses=True
            )
            self.redis_client.ping()
            print(f"[RedisTokenMatcher] Connected to Redis: {redis_host}:{redis_port}")
        except Exception as e:
            print(f"[RedisTokenMatcher] Failed to connect to Redis: {e}")
            self.redis_client = None
        
        # Token ç¬¦å·ç¼“å­˜ï¼ˆç”¨äºå¿«é€ŸåŒ¹é…ï¼‰
        self.token_symbols_cache: Set[str] = set()
        self.token_details_cache: Dict[str, Dict] = {}
        
        # ä¸Šæ¬¡åˆ·æ–°ç¼“å­˜çš„æ—¶é—´
        self.last_cache_refresh = None
        self.cache_refresh_interval = 300  # 5åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡ç¼“å­˜
        
        # åŠ è½½åˆå§‹æ•°æ®
        self._refresh_cache()
    
    def _refresh_cache(self) -> bool:
        """
        ä» Redis åˆ·æ–°æœ¬åœ°ç¼“å­˜
        
        Returns:
            æ˜¯å¦åˆ·æ–°æˆåŠŸ
        """
        if not self.redis_client:
            return False
        
        try:
            print("[RedisTokenMatcher] Refreshing token cache from Redis...")
            
            # è·å–æ‰€æœ‰ token keys
            token_keys = self.redis_client.smembers(self.redis_set_key)
            
            # æå– symbol
            new_symbols = set()
            new_details = {}
            
            for token_key_str in token_keys:
                # token_key_str æ ¼å¼: "symbol:name"
                parts = token_key_str.split(':', 1)
                if len(parts) >= 1:
                    symbol = parts[0]
                    # å¯¹è‹±æ–‡è½¬å¤§å†™ï¼Œä¸­æ–‡ä¿æŒåŸæ ·
                    if symbol.encode('utf-8').isalpha() and symbol.isascii():
                        symbol_normalized = symbol.upper()
                    else:
                        symbol_normalized = symbol
                    
                    new_symbols.add(symbol_normalized)
                    
                    # è·å–è¯¦ç»†ä¿¡æ¯
                    redis_key = f"{self.redis_key_prefix}{token_key_str}"
                    token_data = self.redis_client.hgetall(redis_key)
                    if token_data:
                        new_details[symbol_normalized] = token_data
            
            self.token_symbols_cache = new_symbols
            self.token_details_cache = new_details
            self.last_cache_refresh = datetime.now()
            
            print(f"[RedisTokenMatcher] Cache refreshed: {len(self.token_symbols_cache)} tokens loaded")
            return True
            
        except Exception as e:
            print(f"[RedisTokenMatcher] Failed to refresh cache: {e}")
            return False
    
    def _should_refresh_cache(self) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å­˜
        
        Returns:
            æ˜¯å¦éœ€è¦åˆ·æ–°
        """
        if not self.last_cache_refresh:
            return True
        
        elapsed = (datetime.now() - self.last_cache_refresh).total_seconds()
        return elapsed >= self.cache_refresh_interval
    
    def match_tokens_in_text(self, text: str, auto_refresh=True) -> List[Dict]:
        """
        åœ¨æ–‡æœ¬ä¸­åŒ¹é… Redis ä¸­å·²çŸ¥çš„ token
        
        Args:
            text: è¦åŒ¹é…çš„æ–‡æœ¬
            auto_refresh: æ˜¯å¦è‡ªåŠ¨åˆ·æ–°ç¼“å­˜
            
        Returns:
            åŒ¹é…åˆ°çš„ token åˆ—è¡¨
        """
        if not text:
            return []
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å­˜
        if auto_refresh and self._should_refresh_cache():
            self._refresh_cache()
        
        if not self.token_symbols_cache:
            return []
        
        matched_tokens = []
        
        # æ–¹æ³•1: åŒ¹é… $SYMBOL æ ¼å¼ï¼ˆæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼‰
        dollar_pattern = re.compile(r'\$([A-Za-z0-9\u4e00-\u9fff]+)', re.IGNORECASE)
        dollar_matches = dollar_pattern.findall(text)
        
        for match in dollar_matches:
            # å¯¹è‹±æ–‡ç¬¦å·è½¬å¤§å†™ï¼Œä¸­æ–‡ä¿æŒåŸæ ·
            if match.encode('utf-8').isalpha() and match.isascii():
                symbol_normalized = match.upper()
            else:
                symbol_normalized = match
            
            # æ£€æŸ¥ç¼“å­˜æ—¶ä¹Ÿè¦è€ƒè™‘åŸæ–‡
            if symbol_normalized in self.token_symbols_cache or match in self.token_symbols_cache:
                token_info = {
                    'symbol': match,  # ä¿æŒåŸæ ·ï¼ˆä¸­æ–‡ä¸å˜ï¼‰
                    'matched_text': f'${match}',
                    'match_type': 'dollar_sign',
                    'confidence': 0.9,  # $SYMBOL æ ¼å¼ç½®ä¿¡åº¦é«˜
                    'source': 'redis_matcher'
                }
                
                # æ·»åŠ è¯¦ç»†ä¿¡æ¯ï¼ˆå°è¯•ä¸¤ç§å½¢å¼ï¼‰
                if symbol_normalized in self.token_details_cache:
                    token_info.update(self.token_details_cache[symbol_normalized])
                elif match in self.token_details_cache:
                    token_info.update(self.token_details_cache[match])
                
                matched_tokens.append(token_info)
        
        # æ–¹æ³•2: åŒ¹é…çº¯è‹±æ–‡å¤§å†™å•è¯ï¼ˆéœ€è¦æ›´ä¸¥æ ¼çš„æ¡ä»¶ï¼‰
        word_pattern = re.compile(r'\b([A-Z][A-Z0-9]{1,10})\b')
        word_matches = word_pattern.findall(text)
        
        # åœç”¨è¯ï¼ˆé¿å…è¯¯åŒ¹é…å¸¸è§å¤§å†™è¯ï¼‰
        stopwords = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 
                    'CAN', 'WAS', 'GET', 'GOT', 'NEW', 'NOW', 'OUT', 'DAY',
                    'WHO', 'WHY', 'HOW', 'WHAT', 'WHEN', 'WHERE'}
        
        for match in word_matches:
            symbol_upper = match.upper()
            
            # è·³è¿‡åœç”¨è¯å’Œå·²åŒ¹é…çš„
            if symbol_upper in stopwords:
                continue
            if any(t['symbol'] == symbol_upper for t in matched_tokens):
                continue
            
            if symbol_upper in self.token_symbols_cache:
                # æ£€æŸ¥ä¸Šä¸‹æ–‡ï¼ˆå‘¨å›´æ˜¯å¦æœ‰åŠ å¯†è´§å¸ç›¸å…³è¯æ±‡ï¼‰
                context_score = self._calculate_context_score(text, match)
                
                # åªæœ‰ä¸Šä¸‹æ–‡åˆ†æ•°å¤Ÿé«˜æ‰æ·»åŠ ï¼ˆé¿å…è¯¯æŠ¥ï¼‰
                if context_score >= 0.3:
                    token_info = {
                        'symbol': symbol_upper,
                        'matched_text': match,
                        'match_type': 'word',
                        'confidence': 0.6 + context_score * 0.3,  # æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´ç½®ä¿¡åº¦
                        'context_score': context_score,
                        'source': 'redis_matcher'
                    }
                    
                    # æ·»åŠ è¯¦ç»†ä¿¡æ¯
                    if symbol_upper in self.token_details_cache:
                        token_info.update(self.token_details_cache[symbol_upper])
                    
                    matched_tokens.append(token_info)
        
        # æ–¹æ³•3: åŒ¹é…ä¸­æ–‡ tokenï¼ˆç›´æ¥åŒ¹é…ï¼Œä¸éœ€è¦ $ ç¬¦å·ï¼‰
        # éå†æ‰€æœ‰å·²çŸ¥çš„ä¸­æ–‡ tokenï¼Œçœ‹æ˜¯å¦åœ¨æ–‡æœ¬ä¸­å‡ºç°
        for symbol in self.token_symbols_cache:
            # åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡ tokenï¼ˆåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼‰
            if re.search(r'[\u4e00-\u9fff]', symbol):
                # è·³è¿‡å·²åŒ¹é…çš„
                if any(t['symbol'] == symbol for t in matched_tokens):
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ–‡æœ¬ä¸­
                if symbol in text:
                    # è®¡ç®—ä¸Šä¸‹æ–‡åˆ†æ•°
                    context_score = self._calculate_context_score(text, symbol)
                    
                    # ä¸­æ–‡ token é™ä½é˜ˆå€¼ï¼ˆ0.0ï¼‰ï¼Œå…è®¸æ— ä¸Šä¸‹æ–‡åŒ¹é…
                    # æ³¨æ„ï¼šè¿™ä¼šå¢åŠ è¯¯æŠ¥ç‡ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒè°ƒæ•´ä¸º 0.1 æˆ– 0.2
                    if context_score >= 0.0:
                        token_info = {
                            'symbol': symbol,
                            'matched_text': symbol,
                            'match_type': 'chinese_word',
                            'confidence': 0.7 + context_score * 0.2,  # åŸºç¡€ç½®ä¿¡åº¦ 0.7
                            'context_score': context_score,
                            'source': 'redis_matcher'
                        }
                        
                        # æ·»åŠ è¯¦ç»†ä¿¡æ¯
                        if symbol in self.token_details_cache:
                            token_info.update(self.token_details_cache[symbol])
                        
                        matched_tokens.append(token_info)
        
        return matched_tokens
    
    def _calculate_context_score(self, text: str, token: str, window=50) -> float:
        """
        è®¡ç®— token å‘¨å›´çš„ä¸Šä¸‹æ–‡ç›¸å…³åº¦ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        
        Args:
            text: å®Œæ•´æ–‡æœ¬
            token: è¦æ£€æŸ¥çš„ token
            window: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            
        Returns:
            ä¸Šä¸‹æ–‡åˆ†æ•° (0-1)
        """
        # åŠ å¯†è´§å¸ç›¸å…³å…³é”®è¯ï¼ˆè‹±æ–‡ + ä¸­æ–‡ï¼‰
        crypto_keywords = {
            # è‹±æ–‡å…³é”®è¯
            'crypto', 'token', 'coin', 'blockchain', 'defi', 'nft', 'meme',
            'launch', 'pump', 'moon', 'buy', 'sell', 'trade', 'dex', 'swap',
            'contract', 'address', 'ca', 'launched', '$',
            # ä¸­æ–‡å…³é”®è¯
            'ä»£å¸', 'å¸', 'åŒºå—é“¾', 'åŠ å¯†', 'å‘å¸ƒ', 'ä¸Šçº¿', 'å¯åŠ¨', 'è´­ä¹°',
            'ä¹°å…¥', 'å–å‡º', 'äº¤æ˜“', 'åˆçº¦', 'åœ°å€', 'ç™»æœˆ', 'pump', 'moon',
            'å†²', 'æ¶¨', 'æš´æ¶¨', 'é£', 'èµ·é£', 'æ–°å¸', 'é¡¹ç›®', 'å‘è¡Œ',
            'DexScreener', 'dex', 'pancakeswap'
        }
        
        # æ‰¾åˆ° token åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
        text_lower = text.lower()
        token_lower = token.lower()
        
        try:
            pos = text_lower.index(token_lower)
            
            # æå–ä¸Šä¸‹æ–‡
            start = max(0, pos - window)
            end = min(len(text), pos + len(token) + window)
            context = text_lower[start:end]
            
            # è®¡ç®—å…³é”®è¯å¯†åº¦
            keyword_count = sum(1 for kw in crypto_keywords if kw in context)
            
            # å½’ä¸€åŒ–åˆ†æ•°
            score = min(1.0, keyword_count / 3.0)
            
            return score
            
        except ValueError:
            return 0.0
    
    def get_token_details(self, symbol: str) -> Dict:
        """
        è·å– token çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            symbol: Token ç¬¦å·
            
        Returns:
            Token è¯¦ç»†ä¿¡æ¯å­—å…¸
        """
        symbol_upper = symbol.upper()
        
        # å…ˆæ£€æŸ¥ç¼“å­˜
        if symbol_upper in self.token_details_cache:
            return self.token_details_cache[symbol_upper]
        
        # ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä» Redis è·å–
        if not self.redis_client:
            return {}
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„ token keys
            all_keys = self.redis_client.smembers(self.redis_set_key)
            
            for token_key_str in all_keys:
                if token_key_str.startswith(f"{symbol_upper}:"):
                    redis_key = f"{self.redis_key_prefix}{token_key_str}"
                    token_data = self.redis_client.hgetall(redis_key)
                    if token_data:
                        return token_data
            
            return {}
            
        except Exception as e:
            print(f"[RedisTokenMatcher] Error getting token details: {e}")
            return {}
    
    def force_refresh(self):
        """å¼ºåˆ¶åˆ·æ–°ç¼“å­˜"""
        self._refresh_cache()
    
    def get_cache_stats(self) -> Dict:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            'total_tokens': len(self.token_symbols_cache),
            'last_refresh': self.last_cache_refresh.isoformat() if self.last_cache_refresh else None,
            'cache_age_seconds': (datetime.now() - self.last_cache_refresh).total_seconds() if self.last_cache_refresh else None,
            'redis_connected': self.redis_client is not None
        }


if __name__ == '__main__':
    """æµ‹è¯•æ¨¡å—"""
    
    # åˆ›å»ºåŒ¹é…å™¨
    matcher = RedisTokenMatcher()
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "Just bought $KITKAT! Going to the moon! ğŸš€",
        "New token launch: $BTC is pumping",
        "Check out this MEME coin, it's amazing!",
        "KITKAT token just launched on DexScreener",
        "The weather is nice today"
    ]
    
    print("\n" + "="*70)
    print("Testing Redis Token Matcher")
    print("="*70)
    
    for text in test_texts:
        print(f"\nText: {text}")
        matches = matcher.match_tokens_in_text(text)
        
        if matches:
            print(f"âœ“ Found {len(matches)} match(es):")
            for match in matches:
                print(f"  - {match['symbol']} ({match['match_type']}, confidence: {match['confidence']:.2f})")
        else:
            print("âœ— No matches found")
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    print("\n" + "="*70)
    print("Cache Stats:")
    print("="*70)
    stats = matcher.get_cache_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")

